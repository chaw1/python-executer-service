# å¯ç”¨ç®—å­æ¸…å•

## ðŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£åˆ—å‡ºäº† Python Executor Service ä¸­æ‰€æœ‰å¯ç”¨çš„æ•°æ®æ²»ç†ç®—å­ï¼ˆæ“ä½œï¼‰ã€‚è¿™äº›ç®—å­æŒ‰åŠŸèƒ½åˆ†ç±»ï¼Œæ–¹ä¾¿åœ¨æ™ºèƒ½æ•°æ®æ ‡æ³¨å¹³å°ä¸­é€‰æ‹©ä½¿ç”¨ã€‚

---

## ðŸŽ¯ ç®—å­åˆ†ç±»

### ðŸ“Š ä¸€ã€æ•°æ®åˆ†æžç±»

#### 1.1 åŸºç¡€ç»Ÿè®¡åˆ†æž
```python
# pandas æè¿°æ€§ç»Ÿè®¡
df.describe()           # æ•°å€¼åˆ—ç»Ÿè®¡æ‘˜è¦
df.info()              # æ•°æ®ç±»åž‹å’Œç¼ºå¤±å€¼ä¿¡æ¯
df.value_counts()      # é¢‘æ•°ç»Ÿè®¡
df.corr()              # ç›¸å…³æ€§çŸ©é˜µ
```

**é¢„ç½®æ¨¡æ¿**: `pandas_analysis`

**åŠŸèƒ½**:
- æ•°æ®æ¦‚è§ˆ
- ç»Ÿè®¡æ‘˜è¦ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€å››åˆ†ä½æ•°ç­‰ï¼‰
- æ•°æ®ç±»åž‹æ£€æŸ¥
- ç¼ºå¤±å€¼ç»Ÿè®¡

**ä½¿ç”¨åœºæ™¯**: æ•°æ®ä¸Šä¼ åŽçš„åˆæ­¥è´¨é‡æ£€æŸ¥

---

#### 1.2 é«˜çº§ç»Ÿè®¡åˆ†æž
```python
# scipy ç»Ÿè®¡æ£€éªŒ
from scipy import stats

stats.ttest_ind(data1, data2)    # tæ£€éªŒ
stats.normaltest(data)            # æ­£æ€æ€§æ£€éªŒ
stats.skew(data)                  # ååº¦
stats.kurtosis(data)              # å³°åº¦
stats.pearsonr(x, y)              # çš®å°”é€Šç›¸å…³ç³»æ•°
```

**é¢„ç½®æ¨¡æ¿**: `scipy_stats`

**åŠŸèƒ½**:
- å‡è®¾æ£€éªŒï¼ˆtæ£€éªŒã€å¡æ–¹æ£€éªŒç­‰ï¼‰
- åˆ†å¸ƒæ£€éªŒï¼ˆæ­£æ€æ€§ã€ååº¦ã€å³°åº¦ï¼‰
- ç›¸å…³æ€§åˆ†æž
- ç»Ÿè®¡æŽ¨æ–­

**ä½¿ç”¨åœºæ™¯**: æ•°æ®è´¨é‡æ·±åº¦åˆ†æžã€A/Bæµ‹è¯•

---

### ðŸ§¹ äºŒã€æ•°æ®æ¸…æ´—ç±»

#### 2.1 ç¼ºå¤±å€¼å¤„ç†
```python
# æ£€æµ‹ç¼ºå¤±å€¼
df.isnull().sum()          # ç»Ÿè®¡ç¼ºå¤±å€¼
df.isnull().any()          # æ˜¯å¦æœ‰ç¼ºå¤±

# å¡«å……ç¼ºå¤±å€¼
df.fillna(0)               # å¡«å……0
df.fillna(method='ffill')  # å‰å‘å¡«å……
df.fillna(method='bfill')  # åŽå‘å¡«å……
df.fillna(df.mean())       # å‡å€¼å¡«å……
df.fillna(df.median())     # ä¸­ä½æ•°å¡«å……

# åˆ é™¤ç¼ºå¤±å€¼
df.dropna()                # åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ
df.dropna(axis=1)          # åˆ é™¤å«ç¼ºå¤±å€¼çš„åˆ—
```

**ä½¿ç”¨åœºæ™¯**: æ•°æ®è´¨é‡æå‡ã€è¡¥å…¨ä¸å®Œæ•´æ•°æ®

---

#### 2.2 é‡å¤å€¼å¤„ç†
```python
# æ£€æµ‹é‡å¤
df.duplicated()            # æ ‡è®°é‡å¤è¡Œ
df.duplicated().sum()      # ç»Ÿè®¡é‡å¤æ•°

# åˆ é™¤é‡å¤
df.drop_duplicates()                    # åˆ é™¤æ‰€æœ‰é‡å¤
df.drop_duplicates(subset=['col'])      # åŸºäºŽæŒ‡å®šåˆ—åˆ é™¤
df.drop_duplicates(keep='first')        # ä¿ç•™ç¬¬ä¸€ä¸ª
df.drop_duplicates(keep='last')         # ä¿ç•™æœ€åŽä¸€ä¸ª
```

**ä½¿ç”¨åœºæ™¯**: æ•°æ®åŽ»é‡ã€ä¿è¯æ•°æ®å”¯ä¸€æ€§

---

#### 2.3 å¼‚å¸¸å€¼å¤„ç†
```python
# IQR æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
Q1 = df['column'].quantile(0.25)
Q3 = df['column'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# è¿‡æ»¤å¼‚å¸¸å€¼
df_clean = df[(df['column'] >= lower_bound) &
              (df['column'] <= upper_bound)]

# Z-Score æ–¹æ³•
from scipy import stats
z_scores = np.abs(stats.zscore(df['column']))
df_clean = df[z_scores < 3]
```

**ä½¿ç”¨åœºæ™¯**: æ•°æ®è´¨é‡æŽ§åˆ¶ã€å¼‚å¸¸æ•°æ®æ¸…ç†

---

#### 2.4 æ•°æ®ç±»åž‹è½¬æ¢
```python
# ç±»åž‹è½¬æ¢
df['col'].astype('int')        # è½¬æ•´æ•°
df['col'].astype('float')      # è½¬æµ®ç‚¹
df['col'].astype('str')        # è½¬å­—ç¬¦ä¸²
pd.to_datetime(df['date'])     # è½¬æ—¥æœŸæ—¶é—´
pd.to_numeric(df['col'])       # è½¬æ•°å€¼ï¼ˆè‡ªåŠ¨æŽ¨æ–­ï¼‰

# åˆ†ç±»ç±»åž‹
df['category'].astype('category')
```

**ä½¿ç”¨åœºæ™¯**: æ•°æ®ç±»åž‹è§„èŒƒåŒ–ã€æé«˜å­˜å‚¨æ•ˆçŽ‡

---

### ðŸ”§ ä¸‰ã€æ•°æ®é¢„å¤„ç†ç±»

#### 3.1 æ•°æ®æ ‡å‡†åŒ–ï¼ˆZ-Scoreï¼‰
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)
# å‡å€¼0ï¼Œæ ‡å‡†å·®1
```

**é¢„ç½®æ¨¡æ¿**: `sklearn_preprocessing`

**åŠŸèƒ½**:
- Z-Score æ ‡å‡†åŒ–
- æ¶ˆé™¤é‡çº²å½±å“
- é€‚ç”¨äºŽæ­£æ€åˆ†å¸ƒæ•°æ®

**ä½¿ç”¨åœºæ™¯**: æœºå™¨å­¦ä¹ æ¨¡åž‹è®­ç»ƒå‰çš„ç‰¹å¾ç¼©æ”¾

---

#### 3.2 æ•°æ®å½’ä¸€åŒ–ï¼ˆMin-Maxï¼‰
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df)
# ç¼©æ”¾åˆ° [0, 1] èŒƒå›´
```

**é¢„ç½®æ¨¡æ¿**: `sklearn_preprocessing`

**åŠŸèƒ½**:
- Min-Max å½’ä¸€åŒ–
- ç¼©æ”¾åˆ°æŒ‡å®šèŒƒå›´
- ä¿æŒæ•°æ®åˆ†å¸ƒå½¢çŠ¶

**ä½¿ç”¨åœºæ™¯**: ç¥žç»ç½‘ç»œè¾“å…¥ã€ç‰¹å¾ç¼©æ”¾

---

#### 3.3 æ ‡ç­¾ç¼–ç 
```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# æ ‡ç­¾ç¼–ç ï¼ˆåºå·ï¼‰
encoder = LabelEncoder()
df['category_encoded'] = encoder.fit_transform(df['category'])

# ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hotï¼‰
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(df[['category']])

# pandas ç‹¬çƒ­ç¼–ç 
pd.get_dummies(df['category'])
```

**ä½¿ç”¨åœºæ™¯**: åˆ†ç±»ç‰¹å¾è½¬æ•°å€¼ã€æœºå™¨å­¦ä¹ æ¨¡åž‹è¾“å…¥

---

#### 3.4 æ•°æ®åˆ†å‰²
```python
from sklearn.model_selection import train_test_split

# è®­ç»ƒé›†/æµ‹è¯•é›†åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,      # æµ‹è¯•é›†æ¯”ä¾‹
    random_state=42     # éšæœºç§å­
)
```

**é¢„ç½®æ¨¡æ¿**: `sklearn_preprocessing`

**ä½¿ç”¨åœºæ™¯**: æœºå™¨å­¦ä¹ æ¨¡åž‹è®­ç»ƒå’Œè¯„ä¼°

---

### ðŸ¤– å››ã€æœºå™¨å­¦ä¹ ç±»

#### 4.1 çº¿æ€§å›žå½’
```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

model = LinearRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)

# è¯„ä¼°
r2 = r2_score(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
```

**é¢„ç½®æ¨¡æ¿**: `sklearn_linear_regression`

**åŠŸèƒ½**:
- çº¿æ€§å›žå½’æ¨¡åž‹
- RÂ²ã€MSE è¯„ä¼°
- å¯è§†åŒ–æ‹Ÿåˆçº¿

**ä½¿ç”¨åœºæ™¯**: æ•°å€¼é¢„æµ‹ã€è¶‹åŠ¿åˆ†æž

---

#### 4.2 åˆ†ç±»æ¨¡åž‹
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# éšæœºæ£®æž—åˆ†ç±»
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)
```

**é¢„ç½®æ¨¡æ¿**: `sklearn_classification`

**åŠŸèƒ½**:
- éšæœºæ£®æž—åˆ†ç±»å™¨
- å‡†ç¡®çŽ‡ã€ç²¾ç¡®çŽ‡ã€å¬å›žçŽ‡
- ç‰¹å¾é‡è¦æ€§åˆ†æž

**ä½¿ç”¨åœºæ™¯**: æ•°æ®åˆ†ç±»ã€è¾…åŠ©æ ‡æ³¨

---

#### 4.3 èšç±»åˆ†æž
```python
from sklearn.cluster import KMeans

# K-Means èšç±»
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# èšç±»ä¸­å¿ƒ
centers = kmeans.cluster_centers_
```

**ä½¿ç”¨åœºæ™¯**: æ•°æ®åˆ†ç»„ã€æ— ç›‘ç£æ ‡æ³¨è¾…åŠ©

---

### ðŸ“ˆ äº”ã€æ•°æ®å¯è§†åŒ–ç±»

#### 5.1 Matplotlib åŸºç¡€å›¾è¡¨
```python
import matplotlib.pyplot as plt

# æŠ˜çº¿å›¾
plt.plot(x, y)

# æ•£ç‚¹å›¾
plt.scatter(x, y)

# æŸ±çŠ¶å›¾
plt.bar(categories, values)

# ç›´æ–¹å›¾
plt.hist(data, bins=20)

# é¥¼å›¾
plt.pie(sizes, labels=labels)

plt.show()
```

**é¢„ç½®æ¨¡æ¿**: `matplotlib_basic`

**åŠŸèƒ½**:
- æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€æŸ±çŠ¶å›¾
- ç›´æ–¹å›¾ã€é¥¼å›¾
- åŸºç¡€é…ç½®ï¼ˆæ ‡é¢˜ã€åæ ‡è½´ã€ç½‘æ ¼ï¼‰

**ä½¿ç”¨åœºæ™¯**: å¿«é€Ÿæ•°æ®å¯è§†åŒ–ã€æŠ¥å‘Šç”Ÿæˆ

---

#### 5.2 Seaborn é«˜çº§å¯è§†åŒ–
```python
import seaborn as sns

# æ•£ç‚¹å›¾ï¼ˆå¸¦åˆ†ç±»ï¼‰
sns.scatterplot(data=df, x='x', y='y', hue='category')

# ç®±çº¿å›¾
sns.boxplot(data=df, x='category', y='value')

# å°æç´å›¾
sns.violinplot(data=df, x='category', y='value')

# çƒ­åŠ›å›¾ï¼ˆç›¸å…³æ€§ï¼‰
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')

# åˆ†å¸ƒå›¾
sns.histplot(data=df, x='value', hue='category', kde=True)

# æˆå¯¹å…³ç³»å›¾
sns.pairplot(df, hue='category')
```

**é¢„ç½®æ¨¡æ¿**: `seaborn_visualization`

**åŠŸèƒ½**:
- ç»Ÿè®¡å›¾è¡¨ï¼ˆç®±çº¿å›¾ã€å°æç´å›¾ï¼‰
- ç›¸å…³æ€§çƒ­åŠ›å›¾
- åˆ†å¸ƒå¯è§†åŒ–
- å¤šå˜é‡å…³ç³»å›¾

**ä½¿ç”¨åœºæ™¯**: æ•°æ®æŽ¢ç´¢æ€§åˆ†æžã€ç‰¹å¾å…³ç³»å‘çŽ°

---

#### 5.3 Plotly äº¤äº’å¼å›¾è¡¨
```python
import plotly.express as px
import plotly.graph_objects as go

# äº¤äº’å¼æ•£ç‚¹å›¾
fig = px.scatter(df, x='x', y='y', color='category')

# äº¤äº’å¼æŠ˜çº¿å›¾
fig = px.line(df, x='date', y='value')

# 3D æ•£ç‚¹å›¾
fig = px.scatter_3d(df, x='x', y='y', z='z', color='category')

# åŠ¨æ€æ°”æ³¡å›¾
fig = px.scatter(df, x='x', y='y', size='size',
                 animation_frame='time')

fig.show()
```

**é¢„ç½®æ¨¡æ¿**: `plotly_scatter`

**åŠŸèƒ½**:
- äº¤äº’å¼å›¾è¡¨ï¼ˆç¼©æ”¾ã€æ‚¬åœã€é€‰æ‹©ï¼‰
- 3D å¯è§†åŒ–
- åŠ¨ç”»å›¾è¡¨
- ä»ªè¡¨ç›˜

**ä½¿ç”¨åœºæ™¯**: äº¤äº’å¼æ•°æ®å±•ç¤ºã€å¤æ‚å…³ç³»å¯è§†åŒ–

---

### ðŸ” å…­ã€æ•°æ®è½¬æ¢ç±»

#### 6.1 åˆ†ç»„èšåˆ
```python
# åˆ†ç»„ç»Ÿè®¡
df.groupby('category')['value'].mean()
df.groupby('category')['value'].sum()
df.groupby('category')['value'].count()

# å¤šé‡èšåˆ
df.groupby('category').agg({
    'value1': ['mean', 'sum'],
    'value2': ['min', 'max']
})

# é€è§†è¡¨
pd.pivot_table(df,
               values='value',
               index='row_category',
               columns='col_category',
               aggfunc='mean')
```

**ä½¿ç”¨åœºæ™¯**: æ•°æ®æ±‡æ€»ã€å¤šç»´åº¦åˆ†æž

---

#### 6.2 æ•°æ®åˆå¹¶
```python
# æ¨ªå‘åˆå¹¶
pd.merge(df1, df2, on='key')              # å†…è¿žæŽ¥
pd.merge(df1, df2, on='key', how='left')  # å·¦è¿žæŽ¥
pd.merge(df1, df2, on='key', how='outer') # å¤–è¿žæŽ¥

# çºµå‘åˆå¹¶
pd.concat([df1, df2], axis=0)             # ä¸Šä¸‹æ‹¼æŽ¥
pd.concat([df1, df2], axis=1)             # å·¦å³æ‹¼æŽ¥
```

**ä½¿ç”¨åœºæ™¯**: å¤šæ•°æ®æºæ•´åˆã€æ•°æ®æ‹¼æŽ¥

---

#### 6.3 æ•°æ®é€è§†
```python
# é•¿è½¬å®½
df.pivot(index='id', columns='category', values='value')

# å®½è½¬é•¿
pd.melt(df, id_vars=['id'],
        value_vars=['col1', 'col2'])
```

**ä½¿ç”¨åœºæ™¯**: æ•°æ®æ ¼å¼è½¬æ¢ã€æŠ¥è¡¨åˆ¶ä½œ

---

## ðŸ“¦ é¢„ç½®ä»£ç æ¨¡æ¿ï¼ˆé€šè¿‡ API èŽ·å–ï¼‰

### å¯ç”¨æ¨¡æ¿åˆ—è¡¨

| æ¨¡æ¿åç§° | åŠŸèƒ½æè¿° | ä¸»è¦åº“ |
|---------|---------|--------|
| `matplotlib_basic` | Matplotlib åŸºç¡€å›¾è¡¨ç»˜åˆ¶ | matplotlib, numpy |
| `plotly_scatter` | Plotly äº¤äº’å¼æ•£ç‚¹å›¾ | plotly, pandas |
| `pandas_analysis` | Pandas æ•°æ®åˆ†æž | pandas, numpy |
| `sklearn_preprocessing` | æ•°æ®é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–/å½’ä¸€åŒ–ï¼‰ | sklearn, pandas |
| `sklearn_linear_regression` | çº¿æ€§å›žå½’æ¨¡åž‹ | sklearn, matplotlib |
| `sklearn_classification` | éšæœºæ£®æž—åˆ†ç±» | sklearn |
| `seaborn_visualization` | Seaborn ç»Ÿè®¡å¯è§†åŒ– | seaborn, matplotlib |
| `scipy_stats` | ç»Ÿè®¡æ£€éªŒå’Œåˆ†æž | scipy, matplotlib |

### èŽ·å–æ¨¡æ¿

**API è°ƒç”¨**:
```bash
# èŽ·å–æ‰€æœ‰æ¨¡æ¿
GET http://localhost:8000/templates

# èŽ·å–æŒ‡å®šæ¨¡æ¿
GET http://localhost:8000/templates/pandas_analysis
```

**Python ä»£ç **:
```python
import requests

# èŽ·å–æ‰€æœ‰æ¨¡æ¿
response = requests.get('http://localhost:8000/templates')
templates = response.json()

# èŽ·å–æŒ‡å®šæ¨¡æ¿
response = requests.get('http://localhost:8000/templates/pandas_analysis')
template = response.json()
code = template['code']
```

---

## ðŸ”§ è‡ªå®šä¹‰ç®—å­å¼€å‘

ç”¨æˆ·å¯ä»¥è‡ªç”±ç¼–å†™è‡ªå®šä¹‰çš„æ•°æ®æ²»ç†ä»£ç ï¼Œåªè¦ç¬¦åˆå®‰å…¨è§„èŒƒï¼š

### âœ… å…è®¸çš„æ“ä½œ

**æ•°æ®å¤„ç†**:
```python
import numpy as np
import pandas as pd
from scipy import stats

# æ‰€æœ‰ pandas/numpy/scipy æ“ä½œ
```

**æœºå™¨å­¦ä¹ **:
```python
from sklearn.preprocessing import *
from sklearn.linear_model import *
from sklearn.ensemble import *
from sklearn.cluster import *
from sklearn.metrics import *
from sklearn.model_selection import *
```

**å¯è§†åŒ–**:
```python
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
```

### âŒ ç¦æ­¢çš„æ“ä½œ

```python
# æ–‡ä»¶ç³»ç»Ÿè®¿é—®
open('file.txt')          # âŒ

# ç½‘ç»œè®¿é—®
import requests           # âŒ
import urllib             # âŒ

# ç³»ç»Ÿæ“ä½œ
import os                 # âŒ
import sys                # âŒ
import subprocess         # âŒ

# åŠ¨æ€ä»£ç æ‰§è¡Œ
eval('code')              # âŒ
exec('code')              # âŒ
```

---

## ðŸŽ¯ æŒ‰ä¸šåŠ¡åœºæ™¯é€‰æ‹©ç®—å­

### åœºæ™¯ 1: æ•°æ®è´¨é‡æ£€æŸ¥
**æŽ¨èç®—å­**:
- åŸºç¡€ç»Ÿè®¡åˆ†æž (`df.describe()`)
- ç¼ºå¤±å€¼æ£€æµ‹ (`df.isnull().sum()`)
- é‡å¤å€¼æ£€æµ‹ (`df.duplicated().sum()`)
- æ•°æ®ç±»åž‹æ£€æŸ¥ (`df.dtypes`)

**æ¨¡æ¿**: `pandas_analysis`

---

### åœºæ™¯ 2: æ•°æ®æ¸…æ´—
**æŽ¨èç®—å­**:
- ç¼ºå¤±å€¼å¤„ç† (`fillna`, `dropna`)
- é‡å¤å€¼å¤„ç† (`drop_duplicates`)
- å¼‚å¸¸å€¼å¤„ç† (IQR, Z-Score)
- ç±»åž‹è½¬æ¢ (`astype`)

**è‡ªå®šä¹‰ä»£ç **

---

### åœºæ™¯ 3: æ•°æ®æ ‡å‡†åŒ–
**æŽ¨èç®—å­**:
- StandardScaler (Z-Score æ ‡å‡†åŒ–)
- MinMaxScaler (Min-Max å½’ä¸€åŒ–)

**æ¨¡æ¿**: `sklearn_preprocessing`

---

### åœºæ™¯ 4: æ•°æ®å¯è§†åŒ–
**æŽ¨èç®—å­**:
- Matplotlib åŸºç¡€å›¾è¡¨
- Seaborn ç»Ÿè®¡å›¾è¡¨
- Plotly äº¤äº’å¼å›¾è¡¨

**æ¨¡æ¿**: `matplotlib_basic`, `seaborn_visualization`, `plotly_scatter`

---

### åœºæ™¯ 5: è¾…åŠ©æ ‡æ³¨
**æŽ¨èç®—å­**:
- K-Means èšç±»
- éšæœºæ£®æž—åˆ†ç±»
- ç‰¹å¾é‡è¦æ€§åˆ†æž

**æ¨¡æ¿**: `sklearn_classification` + è‡ªå®šä¹‰èšç±»ä»£ç 

---

### åœºæ™¯ 6: ç»Ÿè®¡åˆ†æž
**æŽ¨èç®—å­**:
- æè¿°æ€§ç»Ÿè®¡ (`describe`)
- ç›¸å…³æ€§åˆ†æž (`corr`)
- å‡è®¾æ£€éªŒ (scipy.stats)
- åˆ†å¸ƒæ£€éªŒ

**æ¨¡æ¿**: `scipy_stats`

---

## ðŸ“Š ç®—å­èƒ½åŠ›çŸ©é˜µ

| ç®—å­ç±»åˆ« | æ•°æ®æ¸…æ´— | æ•°æ®åˆ†æž | æ•°æ®è½¬æ¢ | å¯è§†åŒ– | æœºå™¨å­¦ä¹  |
|---------|---------|---------|---------|-------|---------|
| pandas åŸºç¡€ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­ | â­ |
| numpy | â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­ | â­â­ |
| sklearn é¢„å¤„ç† | â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­ | â­â­â­â­â­ |
| sklearn æ¨¡åž‹ | â­ | â­â­â­â­ | â­â­ | â­ | â­â­â­â­â­ |
| matplotlib | â­ | â­â­ | â­ | â­â­â­â­ | â­ |
| seaborn | â­ | â­â­â­â­ | â­ | â­â­â­â­â­ | â­ |
| plotly | â­ | â­â­â­ | â­ | â­â­â­â­â­ | â­ |
| scipy | â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­ |

---

## ðŸš€ å¿«é€Ÿä½¿ç”¨

### æ–¹å¼ 1: ä½¿ç”¨é¢„ç½®æ¨¡æ¿

```python
import requests

# èŽ·å–æ¨¡æ¿
response = requests.get('http://localhost:8000/templates/pandas_analysis')
code = response.json()['code']

# æ‰§è¡Œæ¨¡æ¿
response = requests.post('http://localhost:8000/execute', json={
    'code': code,
    'datasets': {
        'data.csv': csv_content
    }
})
```

### æ–¹å¼ 2: è‡ªå®šä¹‰ä»£ç 

```python
custom_code = """
import pandas as pd
import numpy as np

# è¯»å–æ•°æ®
df = pd.read_csv('data.csv')

# æ•°æ®æ¸…æ´—
df = df.drop_duplicates()
df = df.fillna(df.mean())

# ç»Ÿè®¡åˆ†æž
print("æ•°æ®è´¨é‡æŠ¥å‘Š:")
print(f"æ€»è¡Œæ•°: {len(df)}")
print(f"ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
print(f"é‡å¤è¡Œ: {df.duplicated().sum()}")

# å¯è§†åŒ–
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
df.hist(bins=20)
plt.tight_layout()
plt.show()
"""

# æ‰§è¡Œ
response = requests.post('http://localhost:8000/execute', json={
    'code': custom_code,
    'datasets': {'data.csv': csv_content}
})
```

---

## ðŸ“š ç›¸å…³æ–‡æ¡£

- **README.md** - é¡¹ç›®æ€»è§ˆ
- **DATASETS_USAGE.md** - æ•°æ®é›†ä¼ é€’åŠŸèƒ½è¯¦è§£
- **TEST_EXAMPLES.md** - æµ‹è¯•ç¤ºä¾‹
- **PROJECT_SUMMARY.md** - é¡¹ç›®æ€»ç»“
- **API æ–‡æ¡£** - http://localhost:8000/docs

---

**ç‰ˆæœ¬**: v1.2.0
**æœ€åŽæ›´æ–°**: 2025-10-31
**æ€»ç®—å­æ•°é‡**: 60+ ä¸ªæ•°æ®æ²»ç†ç®—å­
