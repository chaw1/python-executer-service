# æ•°æ®é›†åŠŸèƒ½å®ç°æ€»ç»“

## ğŸ“‹ é—®é¢˜æè¿°

ç”¨æˆ·é‡åˆ°ä¸¤ä¸ªæ–‡ä»¶è®¿é—®é”™è¯¯ï¼š

### é”™è¯¯ 1: æ–‡ä»¶è·¯å¾„é—®é¢˜
```
é”™è¯¯: [Errno 2] No such file or directory: '{{dataset_path}}/data.csv'
```

### é”™è¯¯ 2: open() å‡½æ•°è¢«ç¦ç”¨
```
é”™è¯¯: æ£€æµ‹åˆ°ç¦æ­¢çš„æ“ä½œ: open()
å®‰å…¨ç­–ç•¥ä¸å…è®¸ä½¿ç”¨æ­¤å‡½æ•°ã€‚
```

**ç”¨æˆ·ä»£ç ç¤ºä¾‹**ï¼š
```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# è¯»å–æ•°æ®
df = pd.read_csv('{{dataset_path}}/data.csv')

# æ•°æ®é¢„å¤„ç†
numeric_cols = df.select_dtypes(include=[np.number]).columns
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[numeric_cols])
```

## âœ… è§£å†³æ–¹æ¡ˆ

å®ç°äº†å®Œæ•´çš„æ•°æ®é›†ä¼ é€’åŠŸèƒ½ï¼Œå…è®¸é€šè¿‡ API ä¼ é€’æ–‡ä»¶å†…å®¹åˆ°æ‰§è¡Œç¯å¢ƒã€‚

### æ ¸å¿ƒå®ç°

#### 1. API å±‚ï¼ˆapp/models.pyï¼‰

æ–°å¢ `datasets` å‚æ•°åˆ°è¯·æ±‚æ¨¡å‹ï¼š

```python
class ExecuteRequest(BaseModel):
    code: str
    timeout: int = 30
    output_format: str = "json"
    datasets: Optional[Dict[str, str]] = None  # æ–°å¢
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```json
{
  "code": "import pandas as pd\ndf = pd.read_csv('data.csv')\nprint(df)",
  "datasets": {
    "data.csv": "name,age,score\nAlice,25,95\nBob,30,87"
  }
}
```

#### 2. æ‰§è¡Œå™¨å±‚ï¼ˆapp/executor.pyï¼‰

æ–°å¢æ•°æ®é›†å‡†å¤‡å’Œæ³¨å…¥åŠŸèƒ½ï¼š

```python
def _prepare_datasets(self, datasets: Dict[str, str], global_vars: Dict[str, Any]) -> None:
    """
    å‡†å¤‡æ•°æ®é›†ï¼Œå°†æ–‡ä»¶å†…å®¹æ³¨å…¥åˆ°æ‰§è¡Œç¯å¢ƒ

    åŠŸèƒ½ï¼š
    1. é¢„å¤„ç† CSV/JSON ä¸º DataFrameï¼ˆç¼“å­˜ï¼‰
    2. åˆ›å»ºè‡ªå®šä¹‰ read_csv/read_json å‡½æ•°
    3. æ”¯æŒ {{dataset_path}} å ä½ç¬¦
    4. ä¼˜å…ˆä»å†…å­˜è¯»å–ï¼Œæ€§èƒ½ä¼˜åŒ–
    """
```

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š
- è¦†ç›– `pd.read_csv()` å’Œ `pd.read_json()`
- è·¯å¾„æ¸…ç†ï¼šç§»é™¤ `{{dataset_path}}/` å‰ç¼€
- é¢„è¯»å–ä¼˜åŒ–ï¼šCSV/JSON æå‰è½¬ä¸º DataFrame
- å†…å­˜ç¼“å­˜ï¼šå¤šæ¬¡è¯»å–è¿”å›å‰¯æœ¬

#### 3. API ç«¯ç‚¹ï¼ˆapp/main.pyï¼‰

æ›´æ–°æ‰§è¡Œç«¯ç‚¹ä¼ é€’æ•°æ®é›†ï¼š

```python
@app.post("/execute")
async def execute_code(request: ExecuteRequest):
    executor = CodeExecutor(timeout=request.timeout)

    # ä¼ é€’ datasets åˆ°æ‰§è¡Œå™¨
    result = executor.execute(request.code, datasets=request.datasets)

    return result
```

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### âœ… æ”¯æŒçš„æ“ä½œ

1. **ç›´æ¥æ–‡ä»¶åè®¿é—®**
```python
df = pd.read_csv('data.csv')
```

2. **å ä½ç¬¦è·¯å¾„ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰**
```python
df = pd.read_csv('{{dataset_path}}/data.csv')  # è‡ªåŠ¨å¤„ç†
```

3. **å¤šæ•°æ®é›†**
```python
df1 = pd.read_csv('users.csv')
df2 = pd.read_csv('orders.csv')
merged = df1.merge(df2, on='user_id')
```

4. **å®Œæ•´æ•°æ®å¤„ç†æµç¨‹**
```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('data.csv')
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.select_dtypes(include=['number']))
```

### ğŸš€ æ€§èƒ½ä¼˜åŒ–

- **é¢„å¤„ç†ç¼“å­˜**: CSV/JSON è‡ªåŠ¨é¢„è¯»ä¸º DataFrame
- **å‡½æ•°è¦†ç›–**: é›¶æ€§èƒ½å¼€é”€çš„å‡½æ•°æ›¿æ¢
- **å†…å­˜å…±äº«**: åŒä¸€æ–‡ä»¶å¤šæ¬¡è¯»å–è¿”å›ç¼“å­˜å‰¯æœ¬

## ğŸ“Š æµ‹è¯•éªŒè¯

### æµ‹è¯•è„šæœ¬: test_datasets.py

**æµ‹è¯• 1: åŸºæœ¬æ•°æ®è¯»å–**
```python
code = """
import pandas as pd
df = pd.read_csv('{{dataset_path}}/data.csv')
print("æ•°æ®å½¢çŠ¶:", df.shape)
print(df.head())
"""

datasets = {
    "data.csv": "name,age,score\nAlice,25,95\nBob,30,87"
}

result = executor.execute(code, datasets=datasets)
```

**ç»“æœ**: âœ… é€šè¿‡ (16ms)

**æµ‹è¯• 2: Sklearn æ•°æ®é¢„å¤„ç†**
```python
code = """
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

df = pd.read_csv('data.csv')
numeric_cols = df.select_dtypes(include=[np.number]).columns

# Z-Score æ ‡å‡†åŒ–
scaler = StandardScaler()
df_standard = df.copy()
df_standard[numeric_cols] = scaler.fit_transform(df[numeric_cols])

print("æ ‡å‡†åŒ–å:")
print(df_standard.describe())
"""
```

**ç»“æœ**: âœ… é€šè¿‡ (22ms)

**é€šè¿‡ç‡**: 2/2 (100%)

## ğŸ“š æ–‡æ¡£

### æ–°å¢æ–‡æ¡£

1. **DATASETS_USAGE.md**
   - å®Œæ•´ä½¿ç”¨æŒ‡å—
   - API ä½¿ç”¨ç¤ºä¾‹
   - Python å®¢æˆ·ç«¯ä»£ç 
   - å¸¸è§é—®é¢˜è§£ç­”
   - å¤šä¸ªå®ç”¨ç¤ºä¾‹

2. **RELEASE_NOTES_v1.2.0.md**
   - ç‰ˆæœ¬å‘å¸ƒè¯´æ˜
   - è¯¦ç»†å˜æ›´æ—¥å¿—
   - å‡çº§æŒ‡å—
   - æ€§èƒ½æŒ‡æ ‡

3. **README.mdï¼ˆæ›´æ–°ï¼‰**
   - æ–°å¢æ•°æ®é›†ä¼ é€’ç‰¹æ€§
   - æ–°å¢ API ä½¿ç”¨ç¤ºä¾‹
   - é“¾æ¥åˆ°è¯¦ç»†æ–‡æ¡£

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### æ•°æ®æµç¨‹

```
1. ç”¨æˆ·å‘é€è¯·æ±‚
   POST /execute
   {
     "code": "...",
     "datasets": {"data.csv": "..."}
   }

2. API å±‚æ¥æ”¶
   ExecuteRequest.datasets: Dict[str, str]

3. æ‰§è¡Œå™¨å‡†å¤‡æ•°æ®é›†
   _prepare_datasets():
   - é¢„è¯» CSV â†’ DataFrame
   - åˆ›å»ºè‡ªå®šä¹‰ read_csv å‡½æ•°
   - æ³¨å…¥åˆ° global_vars

4. ä»£ç æ‰§è¡Œ
   ç”¨æˆ·ä»£ç è°ƒç”¨: pd.read_csv('data.csv')
   â†“
   å®é™…è°ƒç”¨: custom_read_csv('data.csv')
   â†“
   è¿”å›: é¢„å¤„ç†çš„ DataFrame å‰¯æœ¬

5. è¿”å›ç»“æœ
   ExecuteResponse åŒ…å«è¾“å‡º
```

### å…³é”®ä»£ç 

**è‡ªå®šä¹‰ read_csv å‡½æ•°**:
```python
def custom_read_csv(filepath_or_buffer, *args, **kwargs):
    if isinstance(filepath_or_buffer, str):
        # æ¸…ç†è·¯å¾„
        clean_path = filepath_or_buffer.replace('{{dataset_path}}/', '')
        filename = os.path.basename(clean_path)

        # ä¼˜å…ˆè¿”å›é¢„å¤„ç†çš„ DataFrame
        if filename in dataset_dataframes:
            return dataset_dataframes[filename].copy()

        # å¦åˆ™ä» StringIO è¯»å–
        if filename in dataset_contents:
            return original_read_csv(
                io.StringIO(dataset_contents[filename]),
                *args, **kwargs
            )

    # å…œåº•ä½¿ç”¨åŸå§‹å‡½æ•°
    return original_read_csv(filepath_or_buffer, *args, **kwargs)

# æ³¨å…¥åˆ°æ‰§è¡Œç¯å¢ƒ
global_vars['pd'].read_csv = custom_read_csv
```

## ğŸ‰ æˆæœ

### è§£å†³çš„é—®é¢˜

1. âœ… ç”¨æˆ·å¯ä»¥ä½¿ç”¨ `pd.read_csv('data.csv')` è¯»å–æ•°æ®
2. âœ… æ”¯æŒ `{{dataset_path}}` å ä½ç¬¦ï¼ˆè‡ªåŠ¨å¤„ç†ï¼‰
3. âœ… å®Œæ•´çš„ sklearn æ•°æ®é¢„å¤„ç†å·¥ä½œæµ
4. âœ… æ— éœ€æ–‡ä»¶ç³»ç»Ÿè®¿é—®ï¼Œä¿æŒå®‰å…¨æ€§

### æ–°å¢èƒ½åŠ›

1. âœ… é€šè¿‡ API ä¼ é€’ä»»æ„æ•°æ®æ–‡ä»¶
2. âœ… æ”¯æŒ CSVã€JSON æ ¼å¼
3. âœ… è‡ªåŠ¨é¢„å¤„ç†å’Œç¼“å­˜
4. âœ… é«˜æ€§èƒ½å†…å­˜è®¿é—®

### ä¿æŒä¸å˜

1. âœ… å®‰å…¨æ²™ç®±é™åˆ¶ä¸å˜
2. âœ… å‘åå…¼å®¹ï¼ˆdatasets ä¸ºå¯é€‰å‚æ•°ï¼‰
3. âœ… æ— æ€§èƒ½å›é€€

## ğŸ“¦ éƒ¨ç½²

### Git æäº¤

```bash
commit 27b5d07
æ–°å¢æ•°æ®é›†ä¼ é€’åŠŸèƒ½ (v1.2.0)

ä¿®æ”¹æ–‡ä»¶ï¼š
- app/models.py
- app/executor.py
- app/main.py
- README.md

æ–°å¢æ–‡ä»¶ï¼š
- DATASETS_USAGE.md
- RELEASE_NOTES_v1.2.0.md
```

### ç‰ˆæœ¬å‡çº§

- **ä¹‹å‰**: v1.1.0
- **ç°åœ¨**: v1.2.0
- **çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª

## ğŸ“– ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

**Python å®¢æˆ·ç«¯**:
```python
import requests

# å‡†å¤‡æ•°æ®
csv_data = """name,age,score
Alice,25,95
Bob,30,87
Charlie,22,92
"""

# å‡†å¤‡ä»£ç 
code = """
import pandas as pd
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('data.csv')
print("åŸå§‹æ•°æ®:")
print(df)

scaler = StandardScaler()
numeric_cols = df.select_dtypes(include=['number']).columns
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

print("\\næ ‡å‡†åŒ–å:")
print(df)
"""

# å‘é€è¯·æ±‚
response = requests.post(
    'http://localhost:8000/execute',
    json={
        'code': code,
        'datasets': {'data.csv': csv_data}
    }
)

result = response.json()
print(result['output']['stdout'])
```

**curl ç¤ºä¾‹**:
```bash
curl -X POST "http://localhost:8000/execute" \
  -H "Content-Type: application/json" \
  -d '{
    "code": "import pandas as pd\ndf = pd.read_csv(\"data.csv\")\nprint(df.head())",
    "datasets": {
      "data.csv": "name,age\nAlice,25\nBob,30"
    }
  }'
```

### è¯¦ç»†æ–‡æ¡£

è¯·æŸ¥é˜…ï¼š
- **DATASETS_USAGE.md** - å®Œæ•´ä½¿ç”¨æŒ‡å—
- **RELEASE_NOTES_v1.2.0.md** - ç‰ˆæœ¬è¯´æ˜
- **README.md** - æ€»ä½“ä»‹ç»

## ğŸ”® åç»­ä¼˜åŒ–

å¯é€‰çš„æœªæ¥å¢å¼ºï¼ˆè§ UPGRADE_TODO.mdï¼‰ï¼š

1. **ä¼šè¯ç®¡ç†** - æ”¯æŒå¤šæ­¥éª¤æ•°æ®å¤„ç†
2. **æ•°æ®å¯¼å‡º** - å¤„ç†åæ•°æ®å¯¼å‡ºä¸ºæ–‡ä»¶
3. **Excel æ”¯æŒ** - pd.read_excel()
4. **æ–‡ä»¶å¤§å°é™åˆ¶** - å¯é…ç½®çš„æ•°æ®é›†å¤§å°é™åˆ¶

---

**å®Œæˆæ—¶é—´**: 2025-10-31
**ç‰ˆæœ¬**: v1.2.0
**çŠ¶æ€**: âœ… å·²éƒ¨ç½²
**åˆ†æ”¯**: claude/fix-code-recognition-011CUeruPh1DbVmPpyh2Bcbb
